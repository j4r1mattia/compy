import logging


logger = logging.getLogger(__name__)


class Lexer:
    """
    A simple lexer for tokenizing source code.
    A DFA algorithm is used to tokenize the source code.
    The lexer tokenizes the source code by iterating through each character and
    applying state transitions based on a transition table. The current state of
    the lexer is updated based on the character class of the current character.
    The lexer recognizes the following token types:
    - "NUMBER": A numeric literal.
    - "IDENTIFIER": An identifier (variable name).
    - "OPERATOR": An operator (+, -, *, /, <, >, =).
    - "ASSIGN": An assignment operator (=).
    - "IF": The "if" keyword.
    - "ELSE": The "else" keyword.
    - "WHILE": The "while" keyword.
    - "LBRACE": The left brace character "{".
    - "RBRACE": The right brace character "}".
    - "LPAREN": The left parenthesis character "(".
    - "RPAREN": The right parenthesis character ")".
    The lexer logs detailed debug information about the current state, character,
    token, character class, next state, action, and the updated token during the
    tokenization process.
    Attributes:
        source_code (str): The input source code to be tokenized.
        tokens (list): A list of tokens generated by the lexer, where each token
            is a tuple containing the token type, token value, and line number.
        line_number (int): The current line number being processed.
        current_token (str): The token being constructed.
        state (str): The current state of the state machine.
        index (int): The current position in the source code being processed.
        transition_table (dict): A dictionary representing the state transition
            table, mapping (state, char_class) pairs to (next_state, action)
            tuples.
    Raises:
        ValueError: If an invalid character is encountered during tokenization.
"""

    logging.basicConfig(level=logging.INFO)


    # Define the states of the lexer as constants.

    START = "START"
    IN_NUMBER = "IN_NUMBER"
    IN_IDENTIFIER = "IN_IDENTIFIER"
    IN_OPERATOR = "IN_OPERATOR"
    IN_COMMENT = "IN_COMMENT"
    IN_ERROR = "IN_ERROR"


    # Define character classes using bit flags and a lookup table.
    # This allows for fast classification of characters based on their ASCII values.
    # The following character classes are defined:

    UNKOWN      = 0     # 0000
    DIGIT       = 1     # 0001
    ALPHA       = 2     # 0010
    OPERATOR    = 4     # 0100
    WHITESPACE  = 8     # 1000
    NEWLINE     = 16    # 0001 0000
    SINGLE_CHAR = 32    # 0010 0000

    char_classes = [UNKOWN] * 256

    for i in range(ord('0'), ord('9')+1):
        char_classes[i] = DIGIT

    for i in range(ord('a'), ord('z')+1):
        char_classes[i] = ALPHA

    for i in range(ord('A'), ord('Z')+1):
        char_classes[i] = ALPHA

    char_classes[ord("_")] = ALPHA

    for op in "+-*/<>=":
        char_classes[ord(op)] = OPERATOR

    for ws in " \t": 
        char_classes[ord(ws)] = WHITESPACE

    char_classes[ord("\n")] = NEWLINE

    for sc in "{}()":
        char_classes[ord(sc)] = SINGLE_CHAR


    def __init__(self, source_code: str):
        """
        Initialize the lexer with the source code to be tokenized.
        Args:
            source_code (str): The input source code to be tokenized.
        """
        self.source_code = source_code.lstrip()
        self.tokens = []
        self.line_number = 1
        self.current_token = ""
        self.state = self.START
        self.index = 0

        self.transition_table = {
            self.START: {
                self.DIGIT: (self.IN_NUMBER, self._add_to_token),
                self.ALPHA: (self.IN_IDENTIFIER, self._add_to_token),
                self.OPERATOR: (self.IN_OPERATOR, self._add_to_token),
                self.WHITESPACE: (self.START, self._ignore_char),
                self.NEWLINE: (self.START, self._increment_line),
                self.SINGLE_CHAR: (self.START, self._emit_single_character_token),
                self.UNKOWN: (self.START, self._raise_error),
            },
            self.IN_NUMBER: {
                self.DIGIT: (self.IN_NUMBER, self._add_to_token),
                self.WHITESPACE: (self.START, self._emit_number_token),
                self.NEWLINE: (self.START, self._emit_number_token),
                self.UNKOWN: (self.START, self._raise_error),
            },
            self.IN_IDENTIFIER: {
                self.ALPHA: (self.IN_IDENTIFIER, self._add_to_token),
                self.WHITESPACE: (self.START, self._handle_identifier_token),
                self.NEWLINE: (self.START, self._raise_error),
                self.UNKOWN: (self.START, self._raise_error),
            },
            self.IN_OPERATOR: {
                self.OPERATOR: (self.IN_OPERATOR, self._add_to_token),
                self.WHITESPACE: (self.START, self._handle_operator_token),
                self.NEWLINE: (self.START, self._handle_operator_token),
                self.UNKOWN: (self.START, self._raise_error),
            },
        }

    def tokenize(self):
        """
        Tokenize the source code by iterating through each character and applying
        state transitions based on the transition table.
        """

        while self.index < len(self.source_code):
            next_char = self.source_code[self.index]
            char_class = self._get_char_class(next_char)

            logger.debug(f"State: {self.state}, Char: {next_char}, Token: {self.current_token}, Char class: {char_class}")
            self.state, action = self.transition_table[self.state][char_class]
            logger.debug(f"Next state: {self.state}, Action: {action.__name__}")
            self.current_token = action(next_char, self.current_token)
            logger.debug(f"Current token: |{self.current_token}|")
            self.index += 1

    def _get_char_class(self, char: str):
        """
        Get the character class for the given character.
        Args:
            char (str): The character to classify.
        Returns:
            int: The character class as a bit flag.
        """
        char_class = self.char_classes[ord(char)]
        if char_class & self.DIGIT:
            return self.DIGIT
        elif char_class & self.ALPHA:
            return self.ALPHA
        elif char_class & self.OPERATOR:
            return  self.OPERATOR
        elif char_class & self.WHITESPACE:
            return self.WHITESPACE
        elif char_class & self.NEWLINE:
            return self.NEWLINE
        elif char_class & self.SINGLE_CHAR:
            return self.SINGLE_CHAR
        else:
            return self.UNKOWN

    # Following method are a series of actions that the lexer can take
    # based on the current state and the character being processed.

    def _add_to_token(self, char: str, token: str):
        return token + char

    def _ignore_char(self, char: str, token: str):
        return ""

    def _increment_line(self, char: str, token: str):
        self.line_number += 1
        return ""

    def _emit_single_character_token(self, char: str, token: str):
        token_map = {
            "{": "LBRACE",
            "}": "RBRACE",
            "(": "LPAREN",
            ")": "RPAREN",
            "=": "ASSIGN",
        }
        self._emit_token(token_type=token_map[char], token=char)
        return ""

    def _emit_number_token(self, char: str, token: str):
        self._emit_token(token_type="NUMBER", token=token)
        if char == "\n":
            self.line_number += 1
        return ""

    def _handle_identifier_token(self, char: str, token: str):
        if token in ["if", "else", "while"]:
            return self._emit_keyword_token(char, token)
        return self._emit_identifier_token(char, token)

    def _emit_keyword_token(self, char: str, token: str):
        self._emit_token(token_type=token.upper(), token=token)
        if char == "\n":
            self.line_number += 1
        return ""

    def _emit_identifier_token(self, char: str, token: str):
        self._emit_token(token_type="IDENTIFIER", token=token)
        if char == "\n":
            self.line_number += 1
        return ""

    def _handle_operator_token(self, char: str, token: str):
        if token in ["==", "<=", ">=", "!="]:
            token = self._emit_operator_token(token)
        elif token in ["+=", "-=", "*=", "/="]:
            token = self._emit_operator_token(token)
        elif token in ["++", "--", "**"]:
            token = self._emit_operator_token(token)
        elif token == "=":
            token = self._emit_assign_token(token)
        elif token == "//":
            token = self._handle_comment()
        else:
            token = self._emit_operator_token(token)
        return token

    def _emit_operator_token(self, token: str):
        self._emit_token(token_type="OPERATOR", token=token)
        return ""
    

    def _emit_assign_token(self, token: str):
        self._emit_token(token_type="ASSIGN", token=token)
        return ""

    def _emit_token(self, token_type: str, token: str):
        self.tokens.append((token_type, token, self.line_number))

    def _raise_error(self, char: str, token: str):
        raise ValueError(
            f"Invalid character: {char} at line {self.line_number}, index {self.index}"
        )

    def _handle_comment(self):
        comment_end = self.source_code.find("\n", self.index)
        if comment_end == -1:
            self.index = len(self.source_code)
        else:
            self.index = comment_end
        self.line_number += 1
        self.state = self.START
        return ""